{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea47f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Intake\n",
    "# GNN Data Intake ‚Äî Integrity Check & Schema Normalization\n",
    "#\n",
    "# This script will:\n",
    "# 1) Load your provided Excel files (nodes/edges)\n",
    "# 2) Display them as interactive tables\n",
    "# 3) Auto-detect identifier columns (node id, edge src/dst)\n",
    "# 4) Validate referential integrity, nulls, dtypes\n",
    "# 5) Compile a concise validation report\n",
    "# 6) Produce standardized CSVs for downstream GNN code if possible\n",
    "#\n",
    "# Files:\n",
    "# assumed_node_features.xlsx\n",
    "# assumed_edge_features.xlsx\n",
    "#\n",
    "# Outputs:\n",
    "# validation_report.txt\n",
    "# nodes_standardized.csv\n",
    "# edges_standardized.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd038b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f518b6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_id</th>\n",
       "      <th>bond_idx</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>elem1</th>\n",
       "      <th>elem2</th>\n",
       "      <th>pair</th>\n",
       "      <th>distance</th>\n",
       "      <th>bo_mayer_abs</th>\n",
       "      <th>bo_wiberg</th>\n",
       "      <th>bo_mull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.521615</td>\n",
       "      <td>1.019884</td>\n",
       "      <td>1.120686</td>\n",
       "      <td>1.110974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.529253</td>\n",
       "      <td>0.894315</td>\n",
       "      <td>1.078091</td>\n",
       "      <td>1.040698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.096959</td>\n",
       "      <td>0.769069</td>\n",
       "      <td>0.934254</td>\n",
       "      <td>0.697315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C23</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.094622</td>\n",
       "      <td>0.944571</td>\n",
       "      <td>1.120111</td>\n",
       "      <td>0.382928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C23</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.548193</td>\n",
       "      <td>1.062560</td>\n",
       "      <td>1.264060</td>\n",
       "      <td>0.421025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C23</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.098821</td>\n",
       "      <td>0.755087</td>\n",
       "      <td>1.255285</td>\n",
       "      <td>0.930093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C23</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.097661</td>\n",
       "      <td>0.963163</td>\n",
       "      <td>0.788721</td>\n",
       "      <td>0.551803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.569927</td>\n",
       "      <td>0.926406</td>\n",
       "      <td>1.031078</td>\n",
       "      <td>0.717020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C23</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.539477</td>\n",
       "      <td>1.115778</td>\n",
       "      <td>1.071979</td>\n",
       "      <td>0.729561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C23</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.551450</td>\n",
       "      <td>0.858996</td>\n",
       "      <td>1.130484</td>\n",
       "      <td>1.142553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C23</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.574718</td>\n",
       "      <td>1.026572</td>\n",
       "      <td>1.233061</td>\n",
       "      <td>0.562531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C23</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.538221</td>\n",
       "      <td>1.132148</td>\n",
       "      <td>1.431021</td>\n",
       "      <td>0.462602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C23</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.103700</td>\n",
       "      <td>0.836787</td>\n",
       "      <td>0.930235</td>\n",
       "      <td>1.014806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C23</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.560209</td>\n",
       "      <td>0.915566</td>\n",
       "      <td>0.951243</td>\n",
       "      <td>0.812162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C23</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.547107</td>\n",
       "      <td>1.279009</td>\n",
       "      <td>1.037587</td>\n",
       "      <td>0.923971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C23</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C-C</td>\n",
       "      <td>1.590244</td>\n",
       "      <td>1.094151</td>\n",
       "      <td>1.383973</td>\n",
       "      <td>0.745439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C23</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.098387</td>\n",
       "      <td>0.775409</td>\n",
       "      <td>0.977096</td>\n",
       "      <td>0.673142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C23</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.096172</td>\n",
       "      <td>0.904704</td>\n",
       "      <td>1.194663</td>\n",
       "      <td>0.532825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C23</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.090515</td>\n",
       "      <td>0.795539</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.858921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C23</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>C-H</td>\n",
       "      <td>1.092294</td>\n",
       "      <td>0.932799</td>\n",
       "      <td>0.913208</td>\n",
       "      <td>0.932094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mol_id  bond_idx  a1  a2 elem1 elem2 pair  distance  bo_mayer_abs  \\\n",
       "0     C23         0   0   1     C     C  C-C  1.521615      1.019884   \n",
       "1     C23         1   0   5     C     C  C-C  1.529253      0.894315   \n",
       "2     C23         2   0  23     C     H  C-H  1.096959      0.769069   \n",
       "3     C23         3   0  24     C     H  C-H  1.094622      0.944571   \n",
       "4     C23         4   1   2     C     C  C-C  1.548193      1.062560   \n",
       "5     C23         5   1  25     C     H  C-H  1.098821      0.755087   \n",
       "6     C23         6   1  26     C     H  C-H  1.097661      0.963163   \n",
       "7     C23         7   2   3     C     C  C-C  1.569927      0.926406   \n",
       "8     C23         8   2  21     C     C  C-C  1.539477      1.115778   \n",
       "9     C23         9   2  22     C     C  C-C  1.551450      0.858996   \n",
       "10    C23        10   3   4     C     C  C-C  1.574718      1.026572   \n",
       "11    C23        11   3  15     C     C  C-C  1.538221      1.132148   \n",
       "12    C23        12   3  27     C     H  C-H  1.103700      0.836787   \n",
       "13    C23        13   4   5     C     C  C-C  1.560209      0.915566   \n",
       "14    C23        14   4   6     C     C  C-C  1.547107      1.279009   \n",
       "15    C23        15   4   7     C     C  C-C  1.590244      1.094151   \n",
       "16    C23        16   5  28     C     H  C-H  1.098387      0.775409   \n",
       "17    C23        17   5  29     C     H  C-H  1.096172      0.904704   \n",
       "18    C23        18   6  30     C     H  C-H  1.090515      0.795539   \n",
       "19    C23        19   6  31     C     H  C-H  1.092294      0.932799   \n",
       "\n",
       "    bo_wiberg   bo_mull  \n",
       "0    1.120686  1.110974  \n",
       "1    1.078091  1.040698  \n",
       "2    0.934254  0.697315  \n",
       "3    1.120111  0.382928  \n",
       "4    1.264060  0.421025  \n",
       "5    1.255285  0.930093  \n",
       "6    0.788721  0.551803  \n",
       "7    1.031078  0.717020  \n",
       "8    1.071979  0.729561  \n",
       "9    1.130484  1.142553  \n",
       "10   1.233061  0.562531  \n",
       "11   1.431021  0.462602  \n",
       "12   0.930235  1.014806  \n",
       "13   0.951243  0.812162  \n",
       "14   1.037587  0.923971  \n",
       "15   1.383973  0.745439  \n",
       "16   0.977096  0.673142  \n",
       "17   1.194663  0.532825  \n",
       "18   0.892065  0.858921  \n",
       "19   0.913208  0.932094  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NODE_XLSX = Path(\"assumed_node_features.xlsx\")\n",
    "EDGE_XLSX = Path(\"assumed_edge_features.xlsx\")\n",
    "\n",
    "# ---------- 1) Load ----------\n",
    "nodes = pd.read_excel(NODE_XLSX)\n",
    "edges = pd.read_excel(EDGE_XLSX)\n",
    "nodes.head(20)\n",
    "edges.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5b0ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2) Display (raw previews) ----------\n",
    "NODE_ID_LEGACY = \"atom_idx\"   # -> node_id\n",
    "EDGE_SRC_LEGACY = \"a1\"        # -> src\n",
    "EDGE_DST_LEGACY = \"a2\"        # -> dst\n",
    "\n",
    "REMOVE_SELF_LOOPS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54f62a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf205061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GNN Intake Validation Report (alias, undirected, NaN-kept)\n",
      "## Column mappings\n",
      "- graph key: graph_alias\n",
      "- node id: node_id ‚Üê atom_idx\n",
      "- edge ends: src ‚Üê a1, dst ‚Üê a2\n",
      "## Node-ID health\n",
      "- node_id nulls: 0; duplicates (table): 0\n",
      "## Referential integrity (per graph_alias)\n",
      "- ‚ÑπÔ∏è Nodes contain graph_alias with no edges: ['C24']\n",
      "| graph_alias | bad_src | bad_dst | |V| nodes | |E| edges |\n",
      "|---|---:|---:|---:|---:|\n",
      "| C23 | 0 | 0 | 65 | 67 |\n",
      "| C24 | 0 | 0 | 68 | 0 |\n",
      "| C25 | 0 | 0 | 71 | 73 |\n",
      "| C26 | 0 | 0 | 74 | 76 |\n",
      "| C2822R | 0 | 0 | 80 | 82 |\n",
      "| C2822S | 0 | 0 | 74 | 76 |\n",
      "| C29 | 0 | 0 | 79 | 83 |\n",
      "| C2922R | 0 | 0 | 83 | 85 |\n",
      "| C2922S | 0 | 0 | 83 | 85 |\n",
      "| C30 | 0 | 0 | 82 | 86 |\n",
      "| H31R | 0 | 0 | 85 | 89 |\n",
      "| H31S | 0 | 0 | 85 | 89 |\n",
      "| H32R | 0 | 0 | 88 | 92 |\n",
      "| H32S | 0 | 0 | 88 | 92 |\n",
      "| H33R | 0 | 0 | 91 | 95 |\n",
      "| H33S | 0 | 0 | 91 | 95 |\n",
      "| H34R | 0 | 0 | 94 | 98 |\n",
      "| H34S | 0 | 0 | 94 | 98 |\n",
      "| Tm | 0 | 0 | 73 | 77 |\n",
      "| Ts | 0 | 0 | 73 | 77 |\n",
      "- ‚úÖ No invalid edge references found.\n",
      "## Undirected normalization\n",
      "- canonical src<=dst; duplicates removed: 0\n",
      "## Data quality (nulls/dtypes/non-finite)\n",
      "- Top nodes columns by null%:\n",
      "                 dtype  nulls  null_%  nonfinite_count  unique_nonnull  is_constant\n",
      "graph_alias     object      0     0.0                0              20        False\n",
      "atom_idx         int64      0     0.0                0              94        False\n",
      "element         object      0     0.0                0               2        False\n",
      "is_H             int64      0     0.0                0               2        False\n",
      "q_npa          float64      0     0.0                0            1620        False\n",
      "val_mayer_tot  float64      0     0.0                0            1621        False\n",
      "q_hirsh        float64      0     0.0                0            1621        False\n",
      "pop_mull       float64      0     0.0                0            1621        False\n",
      "q_mull         float64      0     0.0                0            1621        False\n",
      "q_adch         float64      0     0.0                0            1621        False\n",
      "- Top edges columns by null%:\n",
      "                dtype  nulls  null_%  nonfinite_count  unique_nonnull  is_constant\n",
      "graph_alias    object      0     0.0                0              19        False\n",
      "bond_idx        int64      0     0.0                0              98        False\n",
      "a1              int64      0     0.0                0              34        False\n",
      "a2              int64      0     0.0                0              93        False\n",
      "elem1          object      0     0.0                0               1         True\n",
      "elem2          object      0     0.0                0               2        False\n",
      "pair           object      0     0.0                0               2        False\n",
      "distance      float64      0     0.0                0            1539        False\n",
      "bo_mayer_abs  float64      0     0.0                0            1615        False\n",
      "bo_wiberg     float64      0     0.0                0            1615        False\n",
      "## NaN policy\n",
      "- NaNs retained at intake; no imputation at this stage.\n",
      "- Later: impute + add missingness masks; fit scalers on observed values only.\n",
      "## Detected numeric feature columns\n",
      "- Node features (count=9): ['atom_idx', 'is_H', 'q_npa', 'val_mayer_tot', 'q_hirsh', 'pop_mull', 'q_mull', 'q_adch', 'q_chelpg']\n",
      "- Edge features (count=7): ['bond_idx', 'a1', 'a2', 'distance', 'bo_mayer_abs', 'bo_wiberg', 'bo_mull']\n",
      "‚úÖ Saved standardized nodes ‚Üí nodes_standardized.csv\n",
      "‚úÖ Saved standardized edges ‚Üí edges_standardized.csv\n"
     ]
    }
   ],
   "source": [
    "# Ê†áÂáÜÂåñË°®Á§∫‚ÄúËäÇÁÇπ ID‚ÄùÂíå‚ÄúËæπÁºòÁ´ØÁÇπ‚ÄùÁöÑÂàóÔºå‰ª•Ë∑®Ë∂ä‰∏ç‰∏ÄËá¥ÁöÑÂëΩÂêç\n",
    "# ---------- Helpers ----------\n",
    "## 1.‰øùÁïôÂéüÂßãÂàóÂêç‰∏ÄËá¥\n",
    "## ‰∏Ä‰∏™Âü∫‰∫éÊ≠£ÂàôË°®ËææÂºèÁöÑÈÄâÊã©Âô® ÔºåÂÆÉÊâ´ÊèèÁé∞Êúâ columns Âπ∂ËøîÂõû‰∏é‰ªª‰Ωï candidates Ê®°ÂºèÂåπÈÖçÁöÑÂàóÔºå‰øùÁïôÈ°∫Â∫èÂπ∂ÈÅøÂÖçÈáçÂ§ç\n",
    "def pick_col(patterns: List[str], columns: pd.Index) -> List[str]:\n",
    "    \"\"\"\n",
    "    Order-preserving regex selector. Returns columns matching any pattern,\n",
    "    preserving pattern priority and avoiding duplicates.\n",
    "    \"\"\"\n",
    "    hits, seen = [], set()\n",
    "    for pat in patterns:\n",
    "        for c in columns:\n",
    "            if re.search(pat, c, flags=re.I) and c not in seen:\n",
    "                hits.append(c); seen.add(c)\n",
    "    return hits\n",
    "\n",
    "def ensure_graph_alias(df: pd.DataFrame) -> Tuple[pd.DataFrame, str, str]:\n",
    "    \"\"\"\n",
    "    Ensure DataFrame has a canonical 'graph_alias' column.\n",
    "    Returns (df_with_alias, alias_col_name, note)\n",
    "    \"\"\"\n",
    "    candidates = pick_col(\n",
    "        [r\"^graph_?alias$\", r\"^graph_?id$\", r\"^mol_?(id|name)$\", r\"^molecule$\", r\"^name$\"],\n",
    "        df.columns\n",
    "    )\n",
    "    note = \"\"\n",
    "    if candidates:\n",
    "        alias = candidates[0]\n",
    "        if alias != \"graph_alias\":\n",
    "            df = df.rename(columns={alias: \"graph_alias\"})\n",
    "    else:\n",
    "        df = df.copy()\n",
    "        df[\"graph_alias\"] = \"MOL_0\"\n",
    "        note = \"No alias column found; created default graph_alias='MOL_0'.\"\n",
    "    return df, \"graph_alias\", note\n",
    "\n",
    "def ensure_zero_based(nodes_df: pd.DataFrame, edges_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Ensure node_id/src/dst are 0-based per graph_alias. If min(node_id)==1 per graph,\n",
    "    shift nodes and corresponding edges by -1. Returns potentially modified frames and a log list.\n",
    "    \"\"\"\n",
    "    log = []\n",
    "    for alias, g_nodes in nodes_df.groupby(\"graph_alias\"):\n",
    "        if g_nodes[\"node_id\"].notna().any():\n",
    "            min_id = g_nodes[\"node_id\"].min()\n",
    "            if pd.notna(min_id) and int(min_id) == 1:\n",
    "                nodes_df.loc[nodes_df[\"graph_alias\"] == alias, \"node_id\"] -= 1\n",
    "                m = edges_df[\"graph_alias\"] == alias\n",
    "                edges_df.loc[m, \"src\"] -= 1\n",
    "                edges_df.loc[m, \"dst\"] -= 1\n",
    "                log.append(f\"- Shifted {alias} from 1-based ‚Üí 0-based.\")\n",
    "    return nodes_df, edges_df, log\n",
    "\n",
    "def quality_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    n_rows = len(df)\n",
    "    nulls = df.isna().sum()\n",
    "    null_pct = (nulls / n_rows * 100).round(2) if n_rows > 0 else 0.0\n",
    "    dtypes = df.dtypes.astype(str)\n",
    "\n",
    "    is_numlike = df.apply(lambda s: pd.api.types.is_numeric_dtype(s))\n",
    "    nonfinite = pd.Series(0, index=df.columns, dtype=\"int64\")\n",
    "    if n_rows > 0:\n",
    "        for c in df.columns[is_numlike]:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            nonfinite[c] = int(s.isna().sum() + np.isinf(s.replace({np.nan: 0})).sum())\n",
    "\n",
    "    nunique_nonnull = df.nunique(dropna=True)\n",
    "    is_constant = (nunique_nonnull <= 1)\n",
    "\n",
    "    q = pd.DataFrame({\n",
    "        \"dtype\": dtypes,\n",
    "        \"nulls\": nulls,\n",
    "        \"null_%\": null_pct,\n",
    "        \"nonfinite_count\": nonfinite,\n",
    "        \"unique_nonnull\": nunique_nonnull,\n",
    "        \"is_constant\": is_constant\n",
    "    }).sort_values([\"null_%\",\"nonfinite_count\"], ascending=[False, False])\n",
    "    return q\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "def main():\n",
    "    report = []\n",
    "    add = report.append\n",
    "\n",
    "    # 0) Load\n",
    "    if not NODE_XLSX.exists():\n",
    "        die(f\"Missing input file: {NODE_XLSX}\")\n",
    "    if not EDGE_XLSX.exists():\n",
    "        die(f\"Missing input file: {EDGE_XLSX}\")\n",
    "\n",
    "    nodes = pd.read_excel(NODE_XLSX)\n",
    "    edges = pd.read_excel(EDGE_XLSX)\n",
    "\n",
    "    # 1) Ensure alias column\n",
    "    nodes_std, alias_nodes, note_nodes = ensure_graph_alias(nodes)\n",
    "    edges_std, alias_edges, note_edges = ensure_graph_alias(edges)\n",
    "    add(\"# GNN Intake Validation Report (alias, undirected, NaN-kept)\")\n",
    "    add(\"## Column mappings\")\n",
    "    if note_nodes: add(f\"- Nodes: {note_nodes}\")\n",
    "    if note_edges: add(f\"- Edges: {note_edges}\")\n",
    "    add(\"- graph key: graph_alias\")\n",
    "    add(f\"- node id: node_id ‚Üê {NODE_ID_LEGACY}\")\n",
    "    add(f\"- edge ends: src ‚Üê {EDGE_SRC_LEGACY}, dst ‚Üê {EDGE_DST_LEGACY}\")\n",
    "\n",
    "    # 2) Canonical IDs\n",
    "    if NODE_ID_LEGACY not in nodes_std.columns:\n",
    "        die(f\"Expected node id column '{NODE_ID_LEGACY}' not found in nodes.\")\n",
    "    if EDGE_SRC_LEGACY not in edges_std.columns or EDGE_DST_LEGACY not in edges_std.columns:\n",
    "        die(f\"Expected edge columns '{EDGE_SRC_LEGACY}'/'{EDGE_DST_LEGACY}' not found in edges.\")\n",
    "\n",
    "    nodes_std[\"node_id\"] = pd.to_numeric(nodes_std[NODE_ID_LEGACY], errors=\"coerce\").astype(\"Int64\")\n",
    "    edges_std[\"src\"]     = pd.to_numeric(edges_std[EDGE_SRC_LEGACY], errors=\"coerce\").astype(\"Int64\")\n",
    "    edges_std[\"dst\"]     = pd.to_numeric(edges_std[EDGE_DST_LEGACY], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # 3) Quick node-id health\n",
    "    dup_count  = nodes_std[[\"graph_alias\",\"node_id\"]].duplicated().sum()\n",
    "    null_count = nodes_std[\"node_id\"].isna().sum()\n",
    "    add(\"## Node-ID health\")\n",
    "    add(f\"- node_id nulls: {int(null_count)}; duplicates (table): {int(dup_count)}\")\n",
    "\n",
    "    # 4) Enforce 0-based per graph\n",
    "    nodes_std, edges_std, zero_logs = ensure_zero_based(nodes_std, edges_std)\n",
    "    if zero_logs:\n",
    "        add(\"## Index normalization\")\n",
    "        add(\"\\n\".join(zero_logs))\n",
    "\n",
    "    # 5) Referential integrity (vectorized, per graph)\n",
    "    # ÊØè‰∏™ËæπÁ´ØÁÇπÂøÖÈ°ªÊåáÂêëÂêå‰∏ÄÂàÜÂ≠ê‰∏≠ÁöÑ‰∏Ä‰∏™ÊúâÊïàËäÇÁÇπ\n",
    "    add(\"## Referential integrity (per graph_alias)\")\n",
    "    rows = []\n",
    "    bad_records = []\n",
    "\n",
    "    aliases_nodes = set(nodes_std[\"graph_alias\"].dropna().unique())\n",
    "    aliases_edges = set(edges_std[\"graph_alias\"].dropna().unique())\n",
    "    missing_in_nodes = sorted(aliases_edges - aliases_nodes)\n",
    "    missing_in_edges = sorted(aliases_nodes - aliases_edges)\n",
    "    if missing_in_nodes:\n",
    "        add(f\"- ‚ö†Ô∏è Edges contain graph_alias with no nodes: {missing_in_nodes[:10]}{' ...' if len(missing_in_nodes)>10 else ''}\")\n",
    "    if missing_in_edges:\n",
    "        add(f\"- ‚ÑπÔ∏è Nodes contain graph_alias with no edges: {missing_in_edges[:10]}{' ...' if len(missing_in_edges)>10 else ''}\")\n",
    "\n",
    "    for alias in sorted(aliases_edges | aliases_nodes):\n",
    "        g_nodes = nodes_std.loc[nodes_std[\"graph_alias\"] == alias, [\"node_id\"]].dropna()\n",
    "        g_edges = edges_std.loc[edges_std[\"graph_alias\"] == alias, [\"graph_alias\",\"src\",\"dst\"]]\n",
    "        V = set(g_nodes[\"node_id\"].astype(\"Int64\").dropna().astype(int).tolist())\n",
    "\n",
    "        if len(g_edges) == 0 and len(g_nodes) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(V) == 0 and len(g_edges) > 0:\n",
    "            rows.append((alias, len(g_edges), len(g_edges), 0, len(g_edges)))\n",
    "            bad = g_edges.copy(); bad[\"reason\"] = \"no_nodes_for_alias\"; bad_records.append(bad); continue\n",
    "\n",
    "        # Mark invalids\n",
    "        sentinel = -10**9\n",
    "        src_invalid_mask = ~g_edges[\"src\"].astype(\"Int64\").fillna(sentinel).astype(int).isin(V)\n",
    "        dst_invalid_mask = ~g_edges[\"dst\"].astype(\"Int64\").fillna(sentinel).astype(int).isin(V)\n",
    "\n",
    "        bad_src = int(src_invalid_mask.sum())\n",
    "        bad_dst = int(dst_invalid_mask.sum())\n",
    "        rows.append((alias, bad_src, bad_dst, len(V), len(g_edges)))\n",
    "\n",
    "        if bad_src:\n",
    "            b = g_edges[src_invalid_mask].copy(); b[\"reason\"] = \"src_not_in_nodes\"; bad_records.append(b)\n",
    "        if bad_dst:\n",
    "            b = g_edges[dst_invalid_mask].copy(); b[\"reason\"] = \"dst_not_in_nodes\"; bad_records.append(b)\n",
    "\n",
    "    add(\"| graph_alias | bad_src | bad_dst | |V| nodes | |E| edges |\")\n",
    "    add(\"|---|---:|---:|---:|---:|\")\n",
    "    for alias, bsrc, bdst, nV, nE in rows:\n",
    "        add(f\"| {alias} | {bsrc} | {bdst} | {nV} | {nE} |\")\n",
    "\n",
    "    if bad_records:\n",
    "        bad_df = pd.concat(bad_records, ignore_index=True)\n",
    "        bad_path = Path(\"invalid_edge_references.csv\")\n",
    "        bad_df.to_csv(bad_path, index=False)\n",
    "        add(f\"- üîç Detailed invalid edges saved to: {bad_path.as_posix()}\")\n",
    "    else:\n",
    "        add(\"- ‚úÖ No invalid edge references found.\")\n",
    "\n",
    "    # 6) Undirected canonicalization & dedup\n",
    "    if REMOVE_SELF_LOOPS:\n",
    "        before = len(edges_std)\n",
    "        edges_std = edges_std[edges_std[\"src\"] != edges_std[\"dst\"]].copy()\n",
    "        add(f\"- Self-loops removed: {before - len(edges_std)}\")\n",
    "\n",
    "    pre_n = len(edges_std)\n",
    "    edges_std[\"src_fix\"] = edges_std[[\"src\",\"dst\"]].min(axis=1).astype(\"Int64\")\n",
    "    edges_std[\"dst_fix\"] = edges_std[[\"src\",\"dst\"]].max(axis=1).astype(\"Int64\")\n",
    "    edges_std = edges_std.drop_duplicates(subset=[\"graph_alias\",\"src_fix\",\"dst_fix\"])\n",
    "    edges_std[[\"src\",\"dst\"]] = edges_std[[\"src_fix\",\"dst_fix\"]]\n",
    "    edges_std = edges_std.drop(columns=[\"src_fix\",\"dst_fix\"])\n",
    "    post_n = len(edges_std)\n",
    "    add(\"## Undirected normalization\")\n",
    "    add(f\"- canonical src<=dst; duplicates removed: {pre_n - post_n}\")\n",
    "\n",
    "    # 7) Data quality overview (no imputation; just reporting)\n",
    "    nodes_q = quality_table(nodes_std)\n",
    "    edges_q = quality_table(edges_std)\n",
    "    # (You can also write nodes_q/edges_q to CSV if you‚Äôd like‚Äîkept in report only.)\n",
    "\n",
    "    add(\"## Data quality (nulls/dtypes/non-finite)\")\n",
    "    # Log a few top offenders by null%\n",
    "    top_nodes = nodes_q.sort_values([\"null_%\",\"nonfinite_count\"], ascending=[False, False]).head(10)\n",
    "    top_edges = edges_q.sort_values([\"null_%\",\"nonfinite_count\"], ascending=[False, False]).head(10)\n",
    "    add(\"- Top nodes columns by null%:\")\n",
    "    add(top_nodes.to_string())\n",
    "    add(\"- Top edges columns by null%:\")\n",
    "    add(top_edges.to_string())\n",
    "\n",
    "    add(\"## NaN policy\")\n",
    "    add(\"- NaNs retained at intake; no imputation at this stage.\")\n",
    "    add(\"- Later: impute + add missingness masks; fit scalers on observed values only.\")\n",
    "\n",
    "    # 8) Feature discovery\n",
    "    def numeric_feature_cols(df: pd.DataFrame, exclude: set) -> list:\n",
    "        cols = []\n",
    "        for c in df.columns:\n",
    "            if c in exclude: \n",
    "                continue\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                cols.append(c)\n",
    "            else:\n",
    "                coerced = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "                if len(df)>0 and coerced.notna().mean() > 0.9:\n",
    "                    cols.append(c)\n",
    "        return cols\n",
    "\n",
    "    exclude_nodes = {\"graph_alias\", \"node_id\"}\n",
    "    exclude_edges = {\"graph_alias\", \"src\", \"dst\"}\n",
    "    node_feat_cols = numeric_feature_cols(nodes_std, exclude_nodes)\n",
    "    edge_feat_cols = numeric_feature_cols(edges_std, exclude_edges)\n",
    "\n",
    "    add(\"## Detected numeric feature columns\")\n",
    "    add(f\"- Node features (count={len(node_feat_cols)}): {node_feat_cols[:24]}{' ...' if len(node_feat_cols)>24 else ''}\")\n",
    "    add(f\"- Edge features (count={len(edge_feat_cols)}): {edge_feat_cols[:24]}{' ...' if len(edge_feat_cols)>24 else ''}\")\n",
    "\n",
    "    # 9) Save standardized outputs (current directory)\n",
    "    saved_nodes = saved_edges = False\n",
    "    if \"node_id\" in nodes_std.columns:\n",
    "        keep = [\"graph_alias\",\"node_id\"] + node_feat_cols\n",
    "        nodes_std[keep].to_csv(\"nodes_standardized.csv\", index=False)\n",
    "        saved_nodes = True\n",
    "    if {\"src\",\"dst\"}.issubset(edges_std.columns):\n",
    "        keep = [\"graph_alias\",\"src\",\"dst\"] + edge_feat_cols\n",
    "        edges_std[keep].to_csv(\"edges_standardized.csv\", index=False)\n",
    "        saved_edges = True\n",
    "\n",
    "    if saved_nodes:\n",
    "        add(\"‚úÖ Saved standardized nodes ‚Üí nodes_standardized.csv\")\n",
    "    else:\n",
    "        add(\"‚ùå Nodes could not be standardized (missing node_id).\")\n",
    "    if saved_edges:\n",
    "        add(\"‚úÖ Saved standardized edges ‚Üí edges_standardized.csv\")\n",
    "    else:\n",
    "        add(\"‚ùå Edges could not be standardized (missing src/dst).\")\n",
    "\n",
    "    # 10) Write report\n",
    "    Path(\"validation_report.txt\").write_text(\"\\n\".join(report), encoding=\"utf-8\")\n",
    "    print(\"\\n\".join(report))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915bac68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43cfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Optional: enforce 0-based indexing for atom indices ----------\n",
    "# RDKit uses 0-based atom indices, but some quantum chemistry outputs use 1-based.\n",
    "# This block automatically checks and shifts if necessary.\n",
    "\n",
    "def ensure_zero_based(nodes_df, edges_df):\n",
    "    \"\"\"Ensure node_id, src, dst start at 0 for each graph_alias.\"\"\"\n",
    "    for alias, g_nodes in nodes_df.groupby(\"graph_alias\"):\n",
    "        min_id = g_nodes[\"node_id\"].min()\n",
    "        if pd.notna(min_id) and min_id == 1:\n",
    "            print(f\"üîß Detected 1-based indexing in nodes for graph {alias} ‚Äî shifting to 0-based.\")\n",
    "            # shift nodes\n",
    "            nodes_df.loc[nodes_df[\"graph_alias\"] == alias, \"node_id\"] -= 1\n",
    "            # shift corresponding edges\n",
    "            mask = edges_df[\"graph_alias\"] == alias\n",
    "            edges_df.loc[mask, \"src\"] -= 1\n",
    "            edges_df.loc[mask, \"dst\"] -= 1\n",
    "    return nodes_df, edges_df\n",
    "\n",
    "# Apply the correction\n",
    "nodes_std, edges_std = ensure_zero_based(nodes_std, edges_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ÊØè‰∏™ËæπÁ´ØÁÇπÂøÖÈ°ªÊåáÂêëÂêå‰∏ÄÂàÜÂ≠ê‰∏≠ÁöÑ‰∏Ä‰∏™ÊúâÊïàËäÇÁÇπ\n",
    "# ---------- 4) Per-graph referential integrity (vectorized) ----------\n",
    "\n",
    "rows = []\n",
    "\n",
    "# 4.1 detect graph_alias sets present in nodes vs edges\n",
    "aliases_nodes = set(nodes_std[\"graph_alias\"].dropna().unique())\n",
    "aliases_edges = set(edges_std[\"graph_alias\"].dropna().unique())\n",
    "\n",
    "missing_in_nodes = sorted(aliases_edges - aliases_nodes)   # edges have graphs that nodes don't\n",
    "missing_in_edges = sorted(aliases_nodes - aliases_edges)   # nodes have graphs that edges don't\n",
    "\n",
    "if missing_in_nodes:\n",
    "    add(f\"- ‚ö†Ô∏è Edges contain graph_alias with **no nodes**: {missing_in_nodes[:10]}{' ...' if len(missing_in_nodes)>10 else ''}\")\n",
    "if missing_in_edges:\n",
    "    add(f\"- ‚ÑπÔ∏è Nodes contain graph_alias with **no edges**: {missing_in_edges[:10]}{' ...' if len(missing_in_edges)>10 else ''}\")\n",
    "\n",
    "# 4.2 summary per graph_alias\n",
    "bad_records = []  # collect bad rows for optional CSV\n",
    "\n",
    "for alias in sorted(aliases_edges | aliases_nodes):\n",
    "    g_nodes = nodes_std.loc[nodes_std[\"graph_alias\"] == alias, [\"node_id\"]].dropna()\n",
    "    g_edges = edges_std.loc[edges_std[\"graph_alias\"] == alias, [\"graph_alias\",\"src\",\"dst\"]]\n",
    "\n",
    "    V = set(g_nodes[\"node_id\"].astype(\"Int64\").dropna().astype(int).tolist())\n",
    "\n",
    "    if len(g_edges) == 0 and len(g_nodes) == 0:\n",
    "        continue  # nothing to check\n",
    "\n",
    "    if len(V) == 0 and len(g_edges) > 0:\n",
    "        # all edges are invalid because the node set is empty\n",
    "        rows.append((alias, len(g_edges), len(g_edges), 0, len(g_edges)))\n",
    "        # stash all bad rows\n",
    "        bad = g_edges.copy()\n",
    "        bad[\"reason\"] = \"no_nodes_for_alias\"\n",
    "        bad_records.append(bad)\n",
    "        continue\n",
    "\n",
    "    # Vectorized membership tests (nullable ints -> coerce, fill with sentinel)\n",
    "    src_ok = g_edges[\"src\"].astype(\"Int64\").dropna().astype(int).isin(V)\n",
    "    dst_ok = g_edges[\"dst\"].astype(\"Int64\").dropna().astype(int).isin(V)\n",
    "\n",
    "    # Align back to original index to mark invalids\n",
    "    src_invalid_mask = ~g_edges[\"src\"].astype(\"Int64\").astype(\"Int64\").fillna(-10**9).astype(int).isin(V)\n",
    "    dst_invalid_mask = ~g_edges[\"dst\"].astype(\"Int64\").astype(\"Int64\").fillna(-10**9).astype(int).isin(V)\n",
    "\n",
    "    bad_src = int(src_invalid_mask.sum())\n",
    "    bad_dst = int(dst_invalid_mask.sum())\n",
    "    rows.append((alias, bad_src, bad_dst, len(V), len(g_edges)))\n",
    "\n",
    "    # stash bad rows with reasons for debugging\n",
    "    if bad_src:\n",
    "        b = g_edges[src_invalid_mask].copy(); b[\"reason\"] = \"src_not_in_nodes\"; bad_records.append(b)\n",
    "    if bad_dst:\n",
    "        b = g_edges[dst_invalid_mask].copy(); b[\"reason\"] = \"dst_not_in_nodes\"; bad_records.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Êó†ÂêëÂõæËæπÊ£ÄÈ™å\n",
    "if {\"src\",\"dst\"}.issubset(edges_std.columns):\n",
    "    edges_std[\"src_fix\"] = edges_std[[\"src\",\"dst\"]].min(axis=1).astype(\"Int64\")\n",
    "    edges_std[\"dst_fix\"] = edges_std[[\"src\",\"dst\"]].max(axis=1).astype(\"Int64\")\n",
    "    pre_n = len(edges_std)\n",
    "    edges_std = edges_std.drop_duplicates(subset=[\"graph_alias\",\"src_fix\",\"dst_fix\"])\n",
    "    post_n = len(edges_std)\n",
    "    add(\"\")\n",
    "    add(\"## Undirected canonicalization\")\n",
    "    add(f\"- Canonical order `src<=dst` enforced. Duplicates removed: {pre_n - post_n}.\")\n",
    "    edges_std[[\"src\",\"dst\"]] = edges_std[[\"src_fix\",\"dst_fix\"]]\n",
    "    edges_std = edges_std.drop(columns=[\"src_fix\",\"dst_fix\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.NAN CHECK\n",
    "def nulls_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    n = df.isna().sum()\n",
    "    pct = (n / len(df) * 100).round(2) if len(df)>0 else 0.0\n",
    "    dtypes = df.dtypes.astype(str)\n",
    "    return pd.DataFrame({\"nulls\": n, \"null_%\": pct, \"dtype\": dtypes}).sort_values(\"null_%\", ascending=False)\n",
    "\n",
    "nodes_nulls = nulls_table(nodes_std)\n",
    "edges_nulls = nulls_table(edges_std)\n",
    "display_dataframe_to_user(\"Nodes ‚Äî nulls & dtypes (post-normalization)\", nodes_nulls)\n",
    "display_dataframe_to_user(\"Edges ‚Äî nulls & dtypes (post-normalization)\", edges_nulls)\n",
    "\n",
    "add(\"\")\n",
    "add(\"## NaN policy (current phase)\")\n",
    "add(\"- NaNs retained as-is. No imputation at intake.\")\n",
    "add(\"- TODO (later): impute + add missingness masks; fit scalers on observed values only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53139c6f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
