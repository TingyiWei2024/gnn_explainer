{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\json_utils.py:187: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:178: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\80710\\anaconda3\\envs\\alfabet-env\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:178: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from alfabet.drawing import draw_mol_outlier\n",
    "from alfabet.fragment import canonicalize_smiles\n",
    "from alfabet.neighbors import find_neighbor_bonds\n",
    "from alfabet.prediction import predict_bdes, check_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import alfabet\n",
    "alfabet.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024.03.5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdkit.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def create_bde_graph_selective_hs(smiles: str, bde_df) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a NetworkX graph from the *original (heavy-atom)* RDKit Mol:\n",
    "      - Keep all heavy-atom ring & skeleton bonds from the SMILES.\n",
    "      - Add new H-X bonds (i.e., only the hydrogens needed) when a row in bde_df indicates\n",
    "        a predicted bond that doesn't already exist in the heavy-atom Mol.\n",
    "    \n",
    "    bde_df is expected to have columns:\n",
    "       - start_atom, end_atom: integer indexes or placeholders\n",
    "       - bde_pred, bdfe_pred, etc.: predicted data for each bond\n",
    "       - possibly bond_index (optional)\n",
    "    \n",
    "    Steps:\n",
    "       1) Parse the SMILES without adding Hs (just once).\n",
    "       2) Build a base Nx graph with all heavy-atom nodes & edges.\n",
    "       3) Iterate over bde_df. If the row corresponds to an existing heavy–heavy bond,\n",
    "          update the Nx edge with predicted data. If the row corresponds to an H–X bond,\n",
    "          add the H node + edge and store the predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Parse the SMILES into an RDKit Mol (no AddHs)\n",
    "    base_mol = Chem.MolFromSmiles(smiles)\n",
    "    if base_mol is None:\n",
    "        # Handle parse error, e.g. return empty graph\n",
    "        return nx.Graph()\n",
    "\n",
    "    # 2. Create an Nx graph, optionally store the RDKit Mol for reference\n",
    "    G = nx.Graph(mol=base_mol)\n",
    "\n",
    "    # 3. Add heavy-atom nodes\n",
    "    #    We'll store:\n",
    "    #      - 'symbol': e.g. 'C', 'O', 'N', etc.\n",
    "    #      - 'rdkit_idx': the integer index assigned by RDKit\n",
    "    #    Feel free to store other attributes as well.\n",
    "    for atom in base_mol.GetAtoms():\n",
    "        atom_idx = atom.GetIdx()\n",
    "        G.add_node(atom_idx, \n",
    "                   symbol=atom.GetSymbol(),\n",
    "                   rdkit_idx=atom_idx)\n",
    "\n",
    "    # 4. Add edges for all heavy-atom bonds in the original (no-H) Mol\n",
    "    #    We won't attach any BDE predictions yet (set them to None).\n",
    "    #    We'll also store a default bond_index=None if desired.\n",
    "    for bond in base_mol.GetBonds():\n",
    "        a1 = bond.GetBeginAtomIdx()\n",
    "        a2 = bond.GetEndAtomIdx()\n",
    "        G.add_edge(a1, a2,\n",
    "                   bond_index=None,\n",
    "                   bde_pred=None,\n",
    "                   bdfe_pred=None)\n",
    "\n",
    "    # 5. Iterate over bde_df.  We'll assume the columns are something like:\n",
    "    #     start_atom, end_atom, bde_pred, bdfe_pred, bond_index, etc.\n",
    "    #    - For heavy–heavy predictions, update the existing edge with predicted data.\n",
    "    #    - For H–X predictions, add the new hydrogen node & edge if not present.\n",
    "    #    - This approach assumes that for an H–X bond, either start_atom or end_atom\n",
    "    #      is a placeholder for hydrogen or an integer representing \"H\" in your dataset.\n",
    "    for _, row in bde_df.iterrows():\n",
    "        s = row['start_atom']\n",
    "        e = row['end_atom']\n",
    "        \n",
    "        # Attempt to interpret s and e in the context of the base mol\n",
    "        # We'll use a simple rule:\n",
    "        #  - If the index is >= base_mol.GetNumAtoms(), treat it as \"this is a hydrogen\"\n",
    "        #  - Or you could have a special marker like -1 for hydrogen\n",
    "        #    (depends on how your data is structured)\n",
    "        \n",
    "        # We also store predicted data\n",
    "        bde_pred_value = row.get('bde_pred', None)\n",
    "        bdfe_pred_value = row.get('bdfe_pred', None)\n",
    "        bond_index_value = row.get('bond_index', None)\n",
    "        \n",
    "        # Convert them to integers if needed\n",
    "        # (In practice, you may need to handle missing or invalid indexes carefully)\n",
    "        \n",
    "        # We'll define a helper function to check if an index is \"heavy\" or \"hydrogen\"\n",
    "        def is_heavy(idx):\n",
    "            return (0 <= idx < base_mol.GetNumAtoms())\n",
    "        \n",
    "        # Determine the \"types\" of s and e\n",
    "        s_is_heavy = is_heavy(s)\n",
    "        e_is_heavy = is_heavy(e)\n",
    "\n",
    "        if s_is_heavy and e_is_heavy:\n",
    "            # This is a heavy–heavy bond.\n",
    "            # If it already exists in G, update attributes.\n",
    "            if G.has_edge(s, e):\n",
    "                # Just update the existing edge\n",
    "                G[s][e]['bde_pred'] = bde_pred_value\n",
    "                G[s][e]['bdfe_pred'] = bdfe_pred_value\n",
    "                G[s][e]['bond_index'] = bond_index_value\n",
    "            else:\n",
    "                # Possibly -?> no, not possible the bond doesn't exist in the original skeleton \n",
    "                # (this can happen if the SMILES didn't have it).\n",
    "                # Add it as a new edge. This is unusual, but let's handle it anyway.\n",
    "                G.add_edge(s, e,\n",
    "                           bond_index=bond_index_value,\n",
    "                           bde_pred=bde_pred_value,\n",
    "                           bdfe_pred=bdfe_pred_value)\n",
    "\n",
    "        else:\n",
    "            # At least one of them is a \"hydrogen\" or out-of-range index\n",
    "            # We'll figure out which one is the heavy atom and which is the hydrogen.\n",
    "            if s_is_heavy and not e_is_heavy:\n",
    "                heavy_idx, hydrogen_idx = s, e\n",
    "            elif e_is_heavy and not s_is_heavy:\n",
    "                heavy_idx, hydrogen_idx = e, s\n",
    "            else:\n",
    "                # Both are hydrogens or out-of-range, which might be invalid.\n",
    "                # For safety, just skip or handle error.\n",
    "                # Could print a warning, raise an exception, etc.\n",
    "                continue\n",
    "\n",
    "            # Step 1: ensure the hydrogen node is present in G\n",
    "            # We'll generate a unique node key for the H, e.g. \"H_{hydrogen_idx}\"\n",
    "            # or something that won't collide with integer-based heavy nodes.\n",
    "            # You could also store the actual integer if your system allows it.\n",
    "            h_node = f\"H_{hydrogen_idx}\"\n",
    "            if not G.has_node(h_node):\n",
    "                # Add the hydrogen node with minimal attributes\n",
    "                G.add_node(h_node,\n",
    "                           symbol='H',\n",
    "                           rdkit_idx=None)  # or some other placeholder\n",
    "\n",
    "            # Step 2: add the H–X bond or update if it already exists\n",
    "            # The heavy_idx is the integer from RDKit.\n",
    "            if not G.has_edge(heavy_idx, h_node):\n",
    "                G.add_edge(heavy_idx, h_node,\n",
    "                           bond_index=bond_index_value,\n",
    "                           bde_pred=bde_pred_value,\n",
    "                           bdfe_pred=bdfe_pred_value)\n",
    "            else:\n",
    "                # If it somehow exists, just update attributes\n",
    "                G[heavy_idx][h_node]['bde_pred'] = bde_pred_value\n",
    "                G[heavy_idx][h_node]['bdfe_pred'] = bdfe_pred_value\n",
    "                G[heavy_idx][h_node]['bond_index'] = bond_index_value\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_df(bde_graph: nx.Graph) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the edges of bde_graph into a DataFrame with columns:\n",
    "      ['u', 'v', 'bond_index', 'graph_bde_pred', 'graph_bdfe_pred'].\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for u, v, data in bde_graph.edges(data=True):\n",
    "        rows.append({\n",
    "            'u': u,\n",
    "            'v': v,\n",
    "            'bond_index': data['bond_index'],\n",
    "            'graph_bde_pred': data.get('bde_pred', None),\n",
    "            'graph_bdfe_pred': data.get('bdfe_pred', None)\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = ['C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CCCC)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CCC(C)C)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@@H](C)CC)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@H](CCC)C)(C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@]2(C)CC3)CC[C@H](C)CCCCC)(C)C',\n",
    "       'C(CCC)C[C@H](C)CC[C@@H]1[C@H](CC[C@H]2[C@]1(CC[C@@H]3[C@@]2(CCCC3(C)C)C)C)C',\n",
    "       'C1CC([C@H]3[C@@](C1)(C)[C@H]2CC[C@H](C)[C@H]([C@@]2(CC3)C)CC[C@@H](CCCC(C)C)C)(C)C',\n",
    "       'C(C[C@@H](CC[C@H]1[C@]3([C@H](CC[C@@H]1C)[C@]2(CCCC(C)(C)[C@@H]2CC3)C)C)C)CC(C)C',\n",
    "       '[C@]23(CC[C@@H]1[C@@](CCCC1(C)C)(C)[C@H]2CC[C@H]4[C@]3(CC[C@]5([C@@H]4CCC5)C)C)C',\n",
    "       '[C@]12(CC[C@@H]5[C@@]([C@H]1CC[C@H]3[C@@]2(C)CC[C@H]4[C@@]3(CCC4)C)(CCCC5(C)C)C)C',\n",
    "       'CC[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCCC(C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C',\n",
    "       'CCCCC[C@@H](C)[C@@H]1CC[C@]2(C1CCC3(C2CCC4C3(CCC5C4(CCCC5(C)C)C)C)C)C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "def quote(x):\n",
    "    return urllib.parse.quote(x, safe='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from rdkit import Chem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Molecule CC[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCCC(C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n",
      "WARNING:root:Molecule CCCCC[C@@H](C)[C@@H]1CC[C@@]2(C)C1CCC1(C)C2CCC2C3(C)CCCC(C)(C)C3CCC21C has undefined stereochemistry\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "graphs = []  # Optionally keep a list of graphs if you want them separately\n",
    "\n",
    "for smiles in smiles_list:\n",
    "    # 1) Canonicalize and sanity-check input\n",
    "    can_smiles = canonicalize_smiles(smiles)\n",
    "    is_outlier, missing_atom, missing_bond = check_input(can_smiles)\n",
    "\n",
    "    # 2) Get DataFrame of predicted BDE/BDFE for each bond\n",
    "    bde_df = predict_bdes(can_smiles, draw=True)\n",
    "    bde_df['raw_smiles'] = smiles\n",
    "\n",
    "    # 3) Deduplicate and store any extra columns you like\n",
    "    bde_df = bde_df.drop_duplicates(['fragment1', 'fragment2']).reset_index(drop=True)\n",
    "    bde_df['smiles_link'] = bde_df.molecule.apply(quote)\n",
    "\n",
    "    # 4) Build a NetworkX graph containing predicted BDE/BDFE\n",
    "    bde_graph = create_bde_graph_selective_hs(can_smiles, bde_df)\n",
    "\n",
    "    # 5) (Optional) store the graph in the DataFrame if you want\n",
    "    #    the same graph for all rows (one per entire molecule)\n",
    "    bde_df['nx_graph'] = [bde_graph] * len(bde_df)\n",
    "\n",
    "    # 6) Append to your results\n",
    "    dfs.append(bde_df)\n",
    "    graphs.append(bde_graph)   # In case you want them in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrame results\n",
    "alfabet_results_022 = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>bond_index</th>\n",
       "      <th>graph_bde_pred</th>\n",
       "      <th>graph_bdfe_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.382645</td>\n",
       "      <td>75.711853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>H_23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.077187</td>\n",
       "      <td>91.049133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.872467</td>\n",
       "      <td>71.412849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>H_27</td>\n",
       "      <td>29.0</td>\n",
       "      <td>97.163109</td>\n",
       "      <td>87.689636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>85.041306</td>\n",
       "      <td>70.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>H_28</td>\n",
       "      <td>30.0</td>\n",
       "      <td>95.392189</td>\n",
       "      <td>86.257256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83.115479</td>\n",
       "      <td>66.995270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>H_30</td>\n",
       "      <td>32.0</td>\n",
       "      <td>94.518456</td>\n",
       "      <td>84.748627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>H_32</td>\n",
       "      <td>34.0</td>\n",
       "      <td>93.767822</td>\n",
       "      <td>84.237808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86.171143</td>\n",
       "      <td>71.943581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>H_33</td>\n",
       "      <td>35.0</td>\n",
       "      <td>93.715736</td>\n",
       "      <td>84.072701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>H_36</td>\n",
       "      <td>38.0</td>\n",
       "      <td>98.747505</td>\n",
       "      <td>89.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>H_37</td>\n",
       "      <td>39.0</td>\n",
       "      <td>95.855804</td>\n",
       "      <td>86.941345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>H_40</td>\n",
       "      <td>42.0</td>\n",
       "      <td>94.971001</td>\n",
       "      <td>85.988342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9</td>\n",
       "      <td>H_41</td>\n",
       "      <td>43.0</td>\n",
       "      <td>86.859306</td>\n",
       "      <td>75.766243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>79.460541</td>\n",
       "      <td>64.253860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>H_44</td>\n",
       "      <td>46.0</td>\n",
       "      <td>97.184883</td>\n",
       "      <td>88.282654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>H_46</td>\n",
       "      <td>48.0</td>\n",
       "      <td>95.408737</td>\n",
       "      <td>86.714401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>H_47</td>\n",
       "      <td>49.0</td>\n",
       "      <td>92.920944</td>\n",
       "      <td>84.154495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14</td>\n",
       "      <td>H_49</td>\n",
       "      <td>51.0</td>\n",
       "      <td>89.283226</td>\n",
       "      <td>79.903214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15.0</td>\n",
       "      <td>82.408630</td>\n",
       "      <td>67.338509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16</td>\n",
       "      <td>H_52</td>\n",
       "      <td>54.0</td>\n",
       "      <td>98.126953</td>\n",
       "      <td>89.117233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>18</td>\n",
       "      <td>H_56</td>\n",
       "      <td>58.0</td>\n",
       "      <td>96.827782</td>\n",
       "      <td>87.840210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>19</td>\n",
       "      <td>H_59</td>\n",
       "      <td>61.0</td>\n",
       "      <td>95.608147</td>\n",
       "      <td>86.592361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20</td>\n",
       "      <td>H_61</td>\n",
       "      <td>63.0</td>\n",
       "      <td>96.082130</td>\n",
       "      <td>87.054794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>21.0</td>\n",
       "      <td>79.644073</td>\n",
       "      <td>64.361122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>22</td>\n",
       "      <td>H_62</td>\n",
       "      <td>64.0</td>\n",
       "      <td>97.762428</td>\n",
       "      <td>88.862091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     u     v  bond_index  graph_bde_pred  graph_bdfe_pred\n",
       "0    0     1         0.0       89.382645        75.711853\n",
       "1    0  H_23        25.0      100.077187        91.049133\n",
       "2    1     2         1.0       85.872467        71.412849\n",
       "3    1  H_27        29.0       97.163109        87.689636\n",
       "4    2     3         2.0       85.041306        70.000275\n",
       "5    2  H_28        30.0       95.392189        86.257256\n",
       "6    3     4         3.0       83.115479        66.995270\n",
       "7    3  H_30        32.0       94.518456        84.748627\n",
       "8    4     5         NaN             NaN              NaN\n",
       "9    4    10         NaN             NaN              NaN\n",
       "10   4  H_32        34.0       93.767822        84.237808\n",
       "11   5     6         5.0       86.171143        71.943581\n",
       "12   5     7         NaN             NaN              NaN\n",
       "13   5  H_33        35.0       93.715736        84.072701\n",
       "14   6  H_36        38.0       98.747505        89.695900\n",
       "15   7     8         NaN             NaN              NaN\n",
       "16   7  H_37        39.0       95.855804        86.941345\n",
       "17   8     9         NaN             NaN              NaN\n",
       "18   8  H_40        42.0       94.971001        85.988342\n",
       "19   9    10         NaN             NaN              NaN\n",
       "20   9    21         NaN             NaN              NaN\n",
       "21   9  H_41        43.0       86.859306        75.766243\n",
       "22  10    11        10.0       79.460541        64.253860\n",
       "23  10    12         NaN             NaN              NaN\n",
       "24  11  H_44        46.0       97.184883        88.282654\n",
       "25  12    13         NaN             NaN              NaN\n",
       "26  12  H_46        48.0       95.408737        86.714401\n",
       "27  13    14         NaN             NaN              NaN\n",
       "28  13  H_47        49.0       92.920944        84.154495\n",
       "29  14    15         NaN             NaN              NaN\n",
       "30  14    21         NaN             NaN              NaN\n",
       "31  14  H_49        51.0       89.283226        79.903214\n",
       "32  15    16        15.0       82.408630        67.338509\n",
       "33  15    17         NaN             NaN              NaN\n",
       "34  15    18         NaN             NaN              NaN\n",
       "35  16  H_52        54.0       98.126953        89.117233\n",
       "36  18    19         NaN             NaN              NaN\n",
       "37  18  H_56        58.0       96.827782        87.840210\n",
       "38  19    20         NaN             NaN              NaN\n",
       "39  19  H_59        61.0       95.608147        86.592361\n",
       "40  20    21         NaN             NaN              NaN\n",
       "41  20  H_61        63.0       96.082130        87.054794\n",
       "42  21    22        21.0       79.644073        64.361122\n",
       "43  22  H_62        64.0       97.762428        88.862091"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_to_df(graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1023 environment rows\n",
      "   temperature  seawater  time component  concentration  degradation_rate\n",
      "0         35.6         1    30       C23             70          0.670914\n",
      "1         35.6         1    30       C24             70          0.680071\n",
      "2         35.6         1    30       C25             70          0.655230\n",
      "3         35.6         1    30       C26             70          0.625193\n",
      "4         35.6         1    30      C28a             70          0.605853\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the environmental data from Excel\n",
    "env_file = r\"C:\\Users\\80710\\OneDrive - Imperial College London\\2025 engineering\\GNN molecules\\graph_pickles\\dataset02.xlsx\"\n",
    "env_df = pd.read_excel(env_file, engine='openpyxl')\n",
    "\n",
    "# Select only the relevant columns for the environment\n",
    "env_columns = [\"temperature\", \"seawater\", \"time\", \"component\",\"concentration\", \"degradation_rate\"]\n",
    "\n",
    "# Ensure all columns exist in the dataset\n",
    "env_var = env_df[env_columns].copy()\n",
    "\n",
    "# Convert categorical \"seawater\" to numerical (if needed)\n",
    "env_var[\"seawater\"] = env_var[\"seawater\"].map({\"sea\": 1, \"art\": 0})  # Map \"sea\" → 1, \"art\" → 0\n",
    "\n",
    "# Drop rows with missing values\n",
    "env_var = env_var.dropna().reset_index(drop=True)\n",
    "\n",
    "# Check if it matches the number of graphs\n",
    "print(f\"Loaded {len(env_var)} environment rows\")\n",
    "print(env_var.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后 DataFrame 列： ['molecule', 'bond_index', 'bond_type', 'start_atom', 'end_atom', 'fragment1', 'fragment2', 'is_valid_stereo', 'bde_pred', 'bdfe_pred', 'bde', 'bdfe', 'set', 'svg', 'has_dft_bde', 'raw_smiles', 'smiles_link', 'nx_graph', 'temperature', 'Concentration', 'Time', 'Seawater', 'degradation_rate', 'concentration', 'time', 'seawater']\n",
      "合并后 DataFrame 大小： (676, 26)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) 先检查长度\n",
    "num_rows = len(alfabet_results_022)\n",
    "if len(env_var) < num_rows:\n",
    "    raise ValueError(\"环境数据行数不足，无法覆盖全部 alfabet_results_022 ！\")\n",
    "\n",
    "# 2) 合并环境数据到 alfabet_results_022\n",
    "alfabet_results_022[\"temperature\"]       = env_var[\"temperature\"][:num_rows].values\n",
    "alfabet_results_022[\"seawater\"]          = env_var[\"seawater\"][:num_rows].values\n",
    "alfabet_results_022[\"time\"]              = env_var[\"time\"][:num_rows].values\n",
    "alfabet_results_022[\"concentration\"]     = env_var[\"concentration\"][:num_rows].values\n",
    "alfabet_results_022[\"degradation_rate\"]  = env_var[\"degradation_rate\"][:num_rows].values\n",
    "\n",
    "print(\"合并后 DataFrame 列：\", alfabet_results_022.columns.tolist())\n",
    "print(\"合并后 DataFrame 大小：\", alfabet_results_022.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MoleculeEnvDataset(Dataset):\n",
    "    \"\"\"\n",
    "    返回 (nx_graph, env_features, target) 形式，用于 GNN 训练。\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        df: 需包含以下列：\n",
    "          - 'nx_graph': 你的 NetworkX 图\n",
    "          - 'temperature', 'seawater', 'time', 'concentration'\n",
    "          - 'degradation_rate'\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "        # 可根据实际列名决定哪些环境变量放进 env_features\n",
    "        self.env_cols = [\"temperature\", \"seawater\", \"time\", \"concentration\"]\n",
    "        self.target_col = \"degradation_rate\"\n",
    "\n",
    "        # 如果需要，可以强制转换为数值型\n",
    "        for col in self.env_cols + [self.target_col]:\n",
    "            self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        self.df = self.df.dropna(subset=self.env_cols + [self.target_col]).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # (1) 取出 NetworkX 图\n",
    "        nx_graph = row[\"nx_graph\"]\n",
    "        \n",
    "        # (2) 环境变量拼成一个张量\n",
    "        env_features = torch.tensor([\n",
    "            row[\"temperature\"],\n",
    "            row[\"seawater\"],\n",
    "            row[\"time\"],\n",
    "            row[\"concentration\"]\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        # (3) target: degradation_rate\n",
    "        target = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "        \n",
    "        return nx_graph, env_features, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    batch 是 [(nx_graph1, env1, tgt1), (nx_graph2, env2, tgt2), ...]\n",
    "    需要把每个元素合并到一起。\n",
    "    返回： (list_of_nx_graphs, env_tensors, target_tensors)\n",
    "    \"\"\"\n",
    "    nx_graph_list  = [b[0] for b in batch]\n",
    "    env_list       = [b[1] for b in batch]\n",
    "    tgt_list       = [b[2] for b in batch]\n",
    "\n",
    "    # env_list / tgt_list 可以用 default_collate 合并成 shape = (batch_size, ...)\n",
    "    env_batch = default_collate(env_list)  # (batch_size, 4) 如果有4个env变量\n",
    "    tgt_batch = default_collate(tgt_list)  # (batch_size,)\n",
    "\n",
    "    return nx_graph_list, env_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleGraphModel(\n",
       "  (env_encoder): EnvPositionalEncoder(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (node_encoder): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (conv1): GCNConv(128, 128)\n",
       "  (conv2): GCNConv(128, 128)\n",
       "  (fc_out): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) 构建 Dataset\n",
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "\n",
    "# 2) DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "# 3) 初始化你的 GNN 模型\n",
    "\n",
    "num_node_features = 1  \n",
    "model = SimpleGraphModel(\n",
    "    num_node_features=num_node_features,\n",
    "    env_input_dim=4,       # temperature, seawater, time, concentration\n",
    "    hidden_dim=128,\n",
    "    output_dim=1\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "\n",
    "def convert_nx_to_pyg(nx_graph):\n",
    "    \"\"\"\n",
    "    直接基于 `create_bde_graph_selective_hs()` 生成的 NetworkX 图转换为 PyG Data 对象。\n",
    "\n",
    "    参数:\n",
    "        nx_graph: NetworkX Graph (包含 'symbol' 作为节点属性, 'bde_pred', 'bdfe_pred' 作为边属性)\n",
    "\n",
    "    返回:\n",
    "        PyG `Data` 对象:\n",
    "            - x: (num_nodes, num_features)  -> 节点特征 (原子类型 One-hot)\n",
    "            - edge_index: (2, num_edges)    -> 边索引\n",
    "            - edge_attr: (num_edges, 2)     -> 边特征 (BDE, BDFE)\n",
    "    \"\"\"\n",
    "    if not isinstance(nx_graph, nx.Graph):\n",
    "        raise ValueError(\"Input must be a valid NetworkX Graph!\")\n",
    "\n",
    "    # **1️⃣ 统一节点索引排序**\n",
    "    int_nodes = [n for n in nx_graph.nodes if isinstance(n, int)]\n",
    "    str_nodes = sorted([n for n in nx_graph.nodes if isinstance(n, str) and n.startswith(\"H_\")],\n",
    "                       key=lambda x: int(x.split(\"_\")[1]))  # \"H_5\" -> 5\n",
    "    nodes = sorted(int_nodes) + str_nodes  # 先排序整数节点，再拼接字符串节点\n",
    "\n",
    "    # **2️⃣ 提取节点特征**\n",
    "    node_features = []\n",
    "    for n in nodes:\n",
    "        symbol = nx_graph.nodes[n].get('symbol', 'C')  # 默认 C\n",
    "        node_features.append([1 if symbol == 'C' else 0])  # 这里用 1 维特征 (是否是C)\n",
    "\n",
    "    x = torch.tensor(node_features, dtype=torch.float)  # (num_nodes, 1)\n",
    "\n",
    "    # **3️⃣ 提取边索引**\n",
    "    edges = list(nx_graph.edges())\n",
    "    edge_index_list = []\n",
    "\n",
    "    for u, v in edges:\n",
    "        if isinstance(u, str):  # 确保 \"H_5\" 这种节点转换为整数索引\n",
    "            u = nodes.index(u)\n",
    "        if isinstance(v, str):\n",
    "            v = nodes.index(v)\n",
    "        edge_index_list.append([u, v])\n",
    "\n",
    "    if len(edge_index_list) > 0:\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).T  # (2, num_edges)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)  # 处理空图\n",
    "\n",
    "    # **4️⃣ 提取边特征，并转换 `None` 为 `0.0`**\n",
    "    edge_features = []\n",
    "    for u, v in edges:\n",
    "        bde_pred = nx_graph[u][v].get(\"bde_pred\", 0.0)  # 确保数值\n",
    "        bdfe_pred = nx_graph[u][v].get(\"bdfe_pred\", 0.0)  # 确保数值\n",
    "\n",
    "        # **确保所有边特征都是 `float` 类型**\n",
    "        bde_pred = float(bde_pred) if bde_pred is not None else 0.0\n",
    "        bdfe_pred = float(bdfe_pred) if bdfe_pred is not None else 0.0\n",
    "\n",
    "        edge_features.append([bde_pred, bdfe_pred])\n",
    "\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float) if edge_features else None\n",
    "\n",
    "    # **5️⃣ 构建 PyG Data**\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total converted PyG graphs: 20\n",
      "Data(x=[42, 1], edge_index=[2, 44], edge_attr=[44, 2])\n"
     ]
    }
   ],
   "source": [
    "# 批量转换所有 NetworkX Graphs -> PyG Data\n",
    "pytorch_graphs = [convert_nx_to_pyg(g) for g in graphs]\n",
    "\n",
    "# 打印转换结果\n",
    "print(f\"Total converted PyG graphs: {len(pytorch_graphs)}\")\n",
    "print(pytorch_graphs[0])  # 查看第一个 PyG Graph 数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def train_loop(model, dataloader, device='cpu', epochs=5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for nx_graph_list, env_batch, tgt_batch in dataloader:\n",
    "            # **1️⃣ 将 NetworkX 列表转换为 PyG Batch**\n",
    "            pyg_data_list = [convert_nx_to_pyg(nxg) for nxg in nx_graph_list]\n",
    "            data_batch = Batch.from_data_list(pyg_data_list).to(device)\n",
    "\n",
    "            # **2️⃣ 送入 GPU**\n",
    "            env_batch = env_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # **3️⃣ 计算前向传播 & 损失**\n",
    "            out = model(data_batch, env_batch)  # (batch_size, )\n",
    "            loss = criterion(out, tgt_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * tgt_batch.size(0)\n",
    "            total_samples += tgt_batch.size(0)\n",
    "\n",
    "        # **4️⃣ 计算 MSE & RMSE**\n",
    "        avg_loss = total_loss / total_samples\n",
    "        rmse = math.sqrt(avg_loss)\n",
    "\n",
    "        # **5️⃣ 计算 R²**\n",
    "        r2_val = evaluate_r2(model, dataloader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - MSE: {avg_loss:.4f}, RMSE: {rmse:.4f}, R²: {r2_val:.4f}\")\n",
    "\n",
    "def evaluate_r2(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    计算 $R^2$ 评分来衡量 GNN 预测结果的拟合度。\n",
    "\n",
    "    参数:\n",
    "    - model: 训练好的 GNN 模型\n",
    "    - dataloader: PyTorch DataLoader (用于验证或测试集)\n",
    "    - device: 设备 ('cpu' 或 'cuda')\n",
    "\n",
    "    返回:\n",
    "    - r2_val: R² 分数\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for nx_graph_list, env_batch, tgt_batch in dataloader:\n",
    "            # **转换 NetworkX → PyG**\n",
    "            pyg_data_list = [convert_nx_to_pyg(nxg) for nxg in nx_graph_list]\n",
    "            data_batch = Batch.from_data_list(pyg_data_list).to(device)\n",
    "\n",
    "            env_batch = env_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "            # **前向传播**\n",
    "            out = model(data_batch, env_batch)\n",
    "\n",
    "            # **存储预测值 & 真实值**\n",
    "            preds.append(out.cpu())\n",
    "            gts.append(tgt_batch.cpu())\n",
    "\n",
    "    # **拼接所有 batch 数据**\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    gts = torch.cat(gts).numpy()\n",
    "\n",
    "    # **计算 R²**\n",
    "    r2_val = r2_score(gts, preds)\n",
    "    return r2_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - MSE Loss: 0.4695\n",
      "Epoch 2/20 - MSE Loss: 0.0753\n",
      "Epoch 3/20 - MSE Loss: 0.0610\n",
      "Epoch 4/20 - MSE Loss: 0.0471\n",
      "Epoch 5/20 - MSE Loss: 0.0637\n",
      "Epoch 6/20 - MSE Loss: 0.0509\n",
      "Epoch 7/20 - MSE Loss: 0.0400\n",
      "Epoch 8/20 - MSE Loss: 0.0423\n",
      "Epoch 9/20 - MSE Loss: 0.0542\n",
      "Epoch 10/20 - MSE Loss: 0.0449\n",
      "Epoch 11/20 - MSE Loss: 0.0471\n",
      "Epoch 12/20 - MSE Loss: 0.0434\n",
      "Epoch 13/20 - MSE Loss: 0.0486\n",
      "Epoch 14/20 - MSE Loss: 0.0557\n",
      "Epoch 15/20 - MSE Loss: 0.0651\n",
      "Epoch 16/20 - MSE Loss: 0.0574\n",
      "Epoch 17/20 - MSE Loss: 0.0425\n",
      "Epoch 18/20 - MSE Loss: 0.0472\n",
      "Epoch 19/20 - MSE Loss: 0.0410\n",
      "Epoch 20/20 - MSE Loss: 0.0599\n"
     ]
    }
   ],
   "source": [
    "# 1) 准备 dataset & dataloader\n",
    "dataset = MoleculeEnvDataset(alfabet_results_022)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "# 2) 初始化模型\n",
    "num_node_features = 1  \n",
    "model = SimpleGraphModel(\n",
    "    num_node_features=num_node_features,\n",
    "    env_input_dim=4,\n",
    "    hidden_dim=128,\n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "# 3) 训练\n",
    "train_loop(model, dataloader, device=device, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - MSE: 0.0618, RMSE: 0.2485, R²: 0.2448\n",
      "Epoch 2/20 - MSE: 0.0464, RMSE: 0.2154, R²: -0.1347\n",
      "Epoch 3/20 - MSE: 0.0826, RMSE: 0.2874, R²: -0.7995\n",
      "Epoch 4/20 - MSE: 0.0571, RMSE: 0.2390, R²: -0.3639\n",
      "Epoch 5/20 - MSE: 0.0452, RMSE: 0.2126, R²: 0.0303\n",
      "Epoch 6/20 - MSE: 0.0540, RMSE: 0.2324, R²: 0.2870\n",
      "Epoch 7/20 - MSE: 0.0424, RMSE: 0.2060, R²: 0.3037\n",
      "Epoch 8/20 - MSE: 0.0394, RMSE: 0.1984, R²: 0.1955\n",
      "Epoch 9/20 - MSE: 0.0437, RMSE: 0.2091, R²: 0.2520\n",
      "Epoch 10/20 - MSE: 0.0403, RMSE: 0.2008, R²: 0.0830\n",
      "Epoch 11/20 - MSE: 0.0433, RMSE: 0.2080, R²: 0.1108\n",
      "Epoch 12/20 - MSE: 0.0401, RMSE: 0.2002, R²: 0.3046\n",
      "Epoch 13/20 - MSE: 0.0386, RMSE: 0.1963, R²: -1.0961\n",
      "Epoch 14/20 - MSE: 0.0492, RMSE: 0.2217, R²: -0.1278\n",
      "Epoch 15/20 - MSE: 0.0417, RMSE: 0.2042, R²: 0.2549\n",
      "Epoch 16/20 - MSE: 0.0400, RMSE: 0.1999, R²: 0.2663\n",
      "Epoch 17/20 - MSE: 0.0366, RMSE: 0.1913, R²: 0.2773\n",
      "Epoch 18/20 - MSE: 0.0420, RMSE: 0.2050, R²: -0.0209\n",
      "Epoch 19/20 - MSE: 0.0396, RMSE: 0.1989, R²: 0.2160\n",
      "Epoch 20/20 - MSE: 0.0377, RMSE: 0.1941, R²: 0.1667\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, dataloader, device=device, epochs=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfabet-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
